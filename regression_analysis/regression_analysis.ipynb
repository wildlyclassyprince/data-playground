{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression Analysis\n",
    "\n",
    "\n",
    "> \"_Nature has established patterns originating in the return of events, but only for the most part. New illnesses flood the human race, so that no matter how many experiments you have done on corpses, you have not thereby imposed a limit on the nature of events so that in the future they could not vary._\"  - **Gottfried Wilhelm Leibniz**\n",
    "\n",
    "\n",
    "This notebook is aimed at showing the different types of regression and how they can be used to solve various problems. There are a couple caveats associated with regression and some common biases, we will only explore a handful of these in our discussion.\n",
    "\n",
    "Before we begin with regresssion, we will take a dive into estimation approaches:\n",
    " \n",
    " - **Ordinary Least Squares (OLS)**\n",
    " - **Maximum Likelihood Expectation (MLE)**\n",
    " - **Bayesian (Univariate & Multivariate)**\n",
    " \n",
    "and we'll make efforts to describe them in greater detail.\n",
    "\n",
    "We will explore several types of regression namely:\n",
    "\n",
    " 1. **Linear Regression**\n",
    " 2. **Ridge Regression**\n",
    " 3. **Lasso Regression**\n",
    " 4. **Bayesian Linear Regression**\n",
    " 5. **Logistic Regression**\n",
    " \n",
    "**NB:** This series is a summary of **Part II: Early Computer-Age Methods** found in **CASI: Computer Age Statistical Inference**.\n",
    "\n",
    "\n",
    "***\n",
    "\n",
    "\n",
    "## [Ordinary Least Squares (OLS)](https://en.wikipedia.org/wiki/Ordinary_least_squares)\n",
    "\n",
    "Ordinary Least Squares (OLS) is a statistical method for approximating the ___unknown parameters___ in a linear regression model by selecting ___parameters___ of linear function from a set of ___explanatory variables___ by the principle of ___least squares___:\n",
    "\n",
    "> *... minimizing the sum of the squares in the differences between the observed ___dependent variable___ (values of the variable being predicted) in the given dataset and those predicted by the linear function ...*\n",
    "\n",
    "OLS is consistent if ___regressors___ are exogenous (i.e., independent of the error term) in the linear model, errors are ___homoscedastic___ (have the same finite variance a.k.a., homogeneity of the variance) and are not correlated. This provides us with the ___minimum-variance mean unbiased___ estimation when the errors have finite variances.\n",
    "\n",
    "If we add the assumption that the errors are normally distributed (i.e., follow a [Gaussian](https://en.wikipedia.org/wiki/Normal_distribution) distribution), OLS is the ___maximum likelihood estimator___.\n",
    "\n",
    "### The linear formulation:\n",
    "\n",
    "Suppose our data has $n$ observations $\\{y_i, x_i\\}^n_{i=1}$, where each observation $i$ includes a scalar response $y_i$ and a column vector $x_j$ of values of $p$ predictors (regressors) $x_{ij}$ for $j = 1, ... , p$. In a linear regression model, the response variable, $y_i$, is a linear function of the regressors:\n",
    "\n",
    "> $y_i = \\beta_1x_{i1} + \\beta_2x_{i2} + ... + \\beta_px_{ip} + \\epsilon_i,$\n",
    "\n",
    "> $y_i = \\bf{X}\\beta + \\epsilon$\n",
    "\n",
    "A measure of the overall model fit is given by the ___Residual Sum of Squares (RSS)___:\n",
    "\n",
    "> $S(b) = \\sum^{n}_{i=1}(y_i - x_i^Tb)^2 = (y - Xb)^T(y - Xb), where T is the traspose.$\n",
    "\n",
    "After estimating $\\beta$, the ___fitted values___ (or ___predicted values___) from the regression will be\n",
    "\n",
    "> $\\hat(y) = X\\hat{\\beta} = Py,$ where $P = X(X^TX)^{-1}X^T$.\n",
    "\n",
    "It is common to assess the goodness-of-fit of the OLS regression by comparing how much the initial variation in the sample can be reduced by regressing onto $X$. The ___coefficient of determination___ $\\bf{R^2}$ is defined as a ratio of \"explained\" variance to the \"total\" variance of the dependent variable $y$:\n",
    "\n",
    "> $R^2 = \\frac{\\sum(\\hat{y_i} - \\bar{y})^2}{\\sum(y_i - \\bar{y})^2} = \\frac{y^TP^TLPy}{y^TLy} = 1 - \\frac{y^TMy}{y^TLy} = 1 - \\frac{RSS}{TSS},$ where TSS is the ___total sum of squares___ for the dependent variable, $L = I_n - \\frac{11^T}{n}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A digression on the assumptions of a regression model\n",
    "\n",
    "An assumption of the fitted model is that the ___standard deviations___ of the error terms are constant and do not depend on the x-value (the predictor) - this is homoscedasticity. It is not required for the estimates to be unbiased, consistent and asymptotically normal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
