{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trump Tweets\n",
    "\n",
    "This the data behind the story [The Worldâ€™s Favorite Donald Trump Tweets](https://fivethirtyeight.com/features/the-worlds-favorite-donald-trump-tweets/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The usual suspects ...\n",
    "import logging\n",
    "import pandas as pd\n",
    "\n",
    "# And their accomplices ...\n",
    "from gensim import corpora\n",
    "from gensim import models\n",
    "from gensim import similarities\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import wordpunct_tokenize\n",
    "from collections import defaultdict\n",
    "from pprint import pprint\n",
    "\n",
    "# Settings\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = pd.read_csv('realDonaldTrump_poll_tweets.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(448, 3)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>created_at</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.656299e+17</td>\n",
       "      <td>8/16/2016 19:22:57</td>\n",
       "      <td>It's just a 2-point race, Clinton 38%, Trump 3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.587319e+17</td>\n",
       "      <td>7/28/2016 18:32:31</td>\n",
       "      <td>\"@LallyRay: Poll: Donald Trump Sees 17-Point P...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.583505e+17</td>\n",
       "      <td>7/27/2016 17:16:56</td>\n",
       "      <td>Great new poll - thank you!\\n#MakeAmericaGreat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7.575775e+17</td>\n",
       "      <td>7/25/2016 14:05:27</td>\n",
       "      <td>Great POLL numbers are coming out all over. Pe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7.536034e+17</td>\n",
       "      <td>7/14/2016 14:53:46</td>\n",
       "      <td>Another new poll. Thank you for your support! ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             id          created_at  \\\n",
       "0  7.656299e+17  8/16/2016 19:22:57   \n",
       "1  7.587319e+17  7/28/2016 18:32:31   \n",
       "2  7.583505e+17  7/27/2016 17:16:56   \n",
       "3  7.575775e+17  7/25/2016 14:05:27   \n",
       "4  7.536034e+17  7/14/2016 14:53:46   \n",
       "\n",
       "                                                text  \n",
       "0  It's just a 2-point race, Clinton 38%, Trump 3...  \n",
       "1  \"@LallyRay: Poll: Donald Trump Sees 17-Point P...  \n",
       "2  Great new poll - thank you!\\n#MakeAmericaGreat...  \n",
       "3  Great POLL numbers are coming out all over. Pe...  \n",
       "4  Another new poll. Thank you for your support! ...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text corpus\n",
    "document = [i for i in tweets['text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing common words and tokenize\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stop_words.update(['-', '=', '+', '*','.', ',', '\"', \"'\", '?', '!', ':', ';', '(', ')', '[', ']', '{', '}'])\n",
    "for doc in document:\n",
    "    list_of_words = [i.lower() for i in wordpunct_tokenize(doc) if i.lower() not in stop_words]\n",
    "stop_words.update(list_of_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['clinton', 'trump'],\n",
      " ['poll:',\n",
      "  'donald',\n",
      "  'trump',\n",
      "  'two',\n",
      "  'breitbart',\n",
      "  '@realdonaldtrump\"',\n",
      "  'great!'],\n",
      " ['great', 'new', 'poll', 'thank', 'you!', '#makeamericagreatagain'],\n",
      " ['great',\n",
      "  'poll',\n",
      "  'numbers',\n",
      "  'coming',\n",
      "  'people',\n",
      "  'want',\n",
      "  'another',\n",
      "  'four',\n",
      "  'years',\n",
      "  'crooked',\n",
      "  'hillary',\n",
      "  'even'],\n",
      " ['another', 'new', 'poll.', 'thank', 'support!', '#imwithyou'],\n",
      " ['great', 'new', 'poll-', 'thank', 'america!', '#trump2016', '#imwithyou'],\n",
      " ['despite', 'spending', 'day', 'ads', 'nationwide', 'zero'],\n",
      " ['great', 'poll-', 'thank', 'you!'],\n",
      " ['new', 'poll', 'thank', 'you!', '#trump2016'],\n",
      " ['new',\n",
      "  'q',\n",
      "  'poll',\n",
      "  'going',\n",
      "  'win',\n",
      "  'make',\n",
      "  'america',\n",
      "  'great',\n",
      "  'again!',\n",
      "  '#trump2016']]\n"
     ]
    }
   ],
   "source": [
    "# Removing common words\n",
    "texts = [[word for word in doc.lower().split() if word not in stop_words] for doc in document]\n",
    "\n",
    "# Removing words that appear only once\n",
    "frequency = defaultdict(int)\n",
    "for text in texts:\n",
    "    for token in text:\n",
    "        frequency[token] += 1\n",
    "\n",
    "texts = [[token for token in text if frequency[token] > 1] for text in texts]\n",
    "\n",
    "# Printing the top 10\n",
    "pprint(texts[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-02-16 20:36:34,257 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-02-16 20:36:34,267 : INFO : built Dictionary(660 unique tokens: ['clinton', 'trump', '@realdonaldtrump\"', 'breitbart', 'donald']...) from 448 documents (total 3742 corpus positions)\n",
      "2019-02-16 20:36:34,267 : INFO : saving Dictionary object under trump.dict, separately None\n",
      "2019-02-16 20:36:34,269 : INFO : saved trump.dict\n",
      "2019-02-16 20:36:34,275 : INFO : storing corpus in Matrix Market format to trump.mm\n",
      "2019-02-16 20:36:34,277 : INFO : saving sparse matrix to trump.mm\n",
      "2019-02-16 20:36:34,278 : INFO : PROGRESS: saving document #0\n",
      "2019-02-16 20:36:34,291 : INFO : saved 448x660 matrix, density=1.239% (3664/295680)\n",
      "2019-02-16 20:36:34,293 : INFO : saving MmCorpus index to trump.mm.index\n"
     ]
    }
   ],
   "source": [
    "# Create dictionary of document\n",
    "bag = corpora.Dictionary(texts)\n",
    "bag.save('trump.dict')\n",
    "\n",
    "# Converting document to a vector (bag-of-words)\n",
    "corpus = [bag.doc2bow(text) for text in texts]\n",
    "corpora.MmCorpus.serialize('trump.mm', corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have assigned a unique integer id to all words appearing in the corpus by:\n",
    "   \n",
    "   1. sweeping across the texts\n",
    "   2. collecting word counts and relevant statistics\n",
    "   \n",
    "Our corpus is a 448 x 661 matrix.\n",
    "\n",
    "***\n",
    "\n",
    "### Transformation: _tf-idf_\n",
    "\n",
    "#### Step 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-02-16 20:36:40,813 : INFO : collecting document frequencies\n",
      "2019-02-16 20:36:40,816 : INFO : PROGRESS: processing document #0\n",
      "2019-02-16 20:36:40,820 : INFO : calculating IDF weights for 448 documents and 659 features (3664 matrix non-zeros)\n"
     ]
    }
   ],
   "source": [
    "# Initialization\n",
    "tfidf = models.TfidfModel(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have initialized (trained) a transaformation model. Different transformation may require different initialization parameters; however, in our case, ___tf-idf___, the \"training\" consists simply of going through the supplied corpus once and computing document frequencies of all its features. This is in comparison to ___Latent Semantic Analysis___ & ___Latent Dirichlet Allocation___ which are more involved and take more time.\n",
    "\n",
    "|Note:|\n",
    "|---|\n",
    "|**A note on transaformations**<br>Transformations always convert between two specific vector spaces. The same vector space (= the same set of feature ids) must be used for training as well as for subsequent vector transformations. Failure to use the same input feature space, such as applying a different string preprocessing, using different feature ids, or using bag-of-words input vectors where ___tf-idf___ vectors are expceted, will result in feature mismatch during transformation calls and consequently in either garbage output and/or runtime exceptions.|\n",
    "\n",
    "#### Step 2:\n",
    "From now on, ___tf-idf___ is treated as a read-only object that can be used to convert any vector from the old representation (___bag-of-words___ integer counts) to the new representation (___tf-idf___ real-valued weights)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying the transformation to the whole corpus\n",
    "corpus_tfidf = tfidf[corpus]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have transformed our corpus (the one we used for training) into a weighted vector. We can do this for any vector (provided they come from the same vector space), even if they are not used in the corpus at all. This can be achived by _folding-in_ for ___LSA___ and by _topic inference_ for ___LDA___.\n",
    "\n",
    "#### Step 3:\n",
    "We will transform our ___tf-idf___ corpus via [Latent Semantic Indexing](https://en.wikipedia.org/wiki/Latent_semantic_indexing) into a latent 10-D space (... num_topics = 10)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-02-16 20:36:44,774 : INFO : using serial LSI version on this node\n",
      "2019-02-16 20:36:44,777 : INFO : updating model with new documents\n",
      "2019-02-16 20:36:44,808 : INFO : preparing a new chunk of documents\n",
      "2019-02-16 20:36:44,816 : INFO : using 100 extra samples and 2 power iterations\n",
      "2019-02-16 20:36:44,817 : INFO : 1st phase: constructing (660, 110) action matrix\n",
      "2019-02-16 20:36:44,844 : INFO : orthonormalizing (660, 110) action matrix\n",
      "2019-02-16 20:36:44,971 : INFO : 2nd phase: running dense svd on (110, 448) matrix\n",
      "2019-02-16 20:36:45,067 : INFO : computing the final decomposition\n",
      "2019-02-16 20:36:45,068 : INFO : keeping 10 factors (discarding 76.499% of energy spectrum)\n",
      "2019-02-16 20:36:45,084 : INFO : processed documents up to #448\n",
      "2019-02-16 20:36:45,086 : INFO : topic #0(3.938): 0.411*\"thank\" + 0.407*\"you!\" + 0.364*\"#makeamericagreatagain\" + 0.346*\"#trump2016\" + 0.305*\"great\" + 0.288*\"new\" + 0.196*\"poll\" + 0.143*\"poll-\" + 0.113*\"poll.\" + 0.111*\"national\"\n",
      "2019-02-16 20:36:45,087 : INFO : topic #1(2.986): 0.402*\"trump\" + -0.241*\"you!\" + 0.227*\"carson\" + 0.227*\"rubio\" + -0.209*\"#makeamericagreatagain\" + -0.204*\"thank\" + 0.191*\"donald\" + 0.184*\"leads\" + 0.182*\"cruz\" + 0.164*\"@realdonaldtrump\"\n",
      "2019-02-16 20:36:45,087 : INFO : topic #2(2.459): -0.499*\"great\" + 0.278*\"trump\" + 0.268*\"#makeamericagreatagain\" + -0.242*\"new\" + -0.211*\"america\" + 0.193*\"#trump2016\" + -0.188*\"make\" + -0.180*\"again!\" + -0.169*\"big\" + 0.162*\"rubio\"\n",
      "2019-02-16 20:36:45,092 : INFO : topic #3(2.292): 0.367*\"national\" + -0.316*\"great\" + -0.257*\"rubio\" + 0.226*\"#trump2016\" + 0.216*\"#makeamericagreatagain\" + 0.215*\"lead\" + -0.207*\"carson\" + -0.201*\"poll-\" + -0.191*\"trump\" + -0.184*\"you!\"\n",
      "2019-02-16 20:36:45,096 : INFO : topic #4(2.171): -0.260*\"national\" + -0.252*\"leads\" + 0.240*\"@realdonaldtrump\" + -0.224*\"donald\" + -0.204*\"great\" + 0.199*\"numbers\" + 0.189*\"@cnn\" + 0.168*\"you!\" + -0.153*\"america\" + 0.152*\"thank\"\n"
     ]
    }
   ],
   "source": [
    "# Initializing an LSI transformation\n",
    "lsi = models.LsiModel(corpus_tfidf, id2word=bag, num_topics=10)\n",
    "corpus_lsi = lsi[corpus_tfidf]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-02-16 20:36:48,853 : INFO : topic #0(3.938): 0.411*\"thank\" + 0.407*\"you!\" + 0.364*\"#makeamericagreatagain\" + 0.346*\"#trump2016\" + 0.305*\"great\" + 0.288*\"new\" + 0.196*\"poll\" + 0.143*\"poll-\" + 0.113*\"poll.\" + 0.111*\"national\"\n",
      "2019-02-16 20:36:48,856 : INFO : topic #1(2.986): 0.402*\"trump\" + -0.241*\"you!\" + 0.227*\"carson\" + 0.227*\"rubio\" + -0.209*\"#makeamericagreatagain\" + -0.204*\"thank\" + 0.191*\"donald\" + 0.184*\"leads\" + 0.182*\"cruz\" + 0.164*\"@realdonaldtrump\"\n",
      "2019-02-16 20:36:48,858 : INFO : topic #2(2.459): -0.499*\"great\" + 0.278*\"trump\" + 0.268*\"#makeamericagreatagain\" + -0.242*\"new\" + -0.211*\"america\" + 0.193*\"#trump2016\" + -0.188*\"make\" + -0.180*\"again!\" + -0.169*\"big\" + 0.162*\"rubio\"\n",
      "2019-02-16 20:36:48,859 : INFO : topic #3(2.292): 0.367*\"national\" + -0.316*\"great\" + -0.257*\"rubio\" + 0.226*\"#trump2016\" + 0.216*\"#makeamericagreatagain\" + 0.215*\"lead\" + -0.207*\"carson\" + -0.201*\"poll-\" + -0.191*\"trump\" + -0.184*\"you!\"\n",
      "2019-02-16 20:36:48,861 : INFO : topic #4(2.171): -0.260*\"national\" + -0.252*\"leads\" + 0.240*\"@realdonaldtrump\" + -0.224*\"donald\" + -0.204*\"great\" + 0.199*\"numbers\" + 0.189*\"@cnn\" + 0.168*\"you!\" + -0.153*\"america\" + 0.152*\"thank\"\n",
      "2019-02-16 20:36:48,862 : INFO : topic #5(2.126): 0.323*\"rubio\" + 0.312*\"carson\" + -0.286*\"donald\" + 0.260*\"cruz\" + -0.195*\"clinton\" + -0.182*\"poll:\" + -0.177*\"leads\" + -0.177*\"@realdonaldtrump\" + 0.176*\"national\" + 0.150*\"bush\"\n",
      "2019-02-16 20:36:48,863 : INFO : topic #6(2.021): 0.318*\"new\" + 0.265*\"@realdonaldtrump\" + -0.243*\"big\" + 0.213*\"great\" + -0.181*\"you!\" + -0.178*\"hillary\" + -0.175*\"clinton\" + 0.162*\"#1\" + -0.151*\"america\" + -0.143*\"leads\"\n",
      "2019-02-16 20:36:48,865 : INFO : topic #7(1.979): -0.276*\"debate\" + 0.235*\"poll.\" + -0.223*\"said\" + 0.210*\"iowa\" + 0.208*\"lead\" + 0.197*\"new\" + -0.192*\"debate.\" + 0.183*\"@cnn\" + -0.181*\"every\" + -0.155*\"great\"\n",
      "2019-02-16 20:36:48,868 : INFO : topic #8(1.925): 0.345*\"@realdonaldtrump\" + -0.220*\"debate\" + 0.200*\"big\" + -0.198*\"last\" + 0.197*\"leads\" + -0.191*\"national\" + -0.189*\"lead\" + 0.173*\"leading\" + 0.151*\"#1\" + -0.148*\"said\"\n",
      "2019-02-16 20:36:48,869 : INFO : topic #9(1.908): -0.327*\"leads\" + 0.265*\"numbers\" + 0.253*\"lead\" + -0.207*\"new\" + -0.195*\"debate.\" + 0.167*\"clinton\" + -0.147*\"poll.\" + -0.146*\"said\" + 0.139*\"poll:\" + 0.136*\"hillary\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.411*\"thank\" + 0.407*\"you!\" + 0.364*\"#makeamericagreatagain\" + 0.346*\"#trump2016\" + 0.305*\"great\" + 0.288*\"new\" + 0.196*\"poll\" + 0.143*\"poll-\" + 0.113*\"poll.\" + 0.111*\"national\"'),\n",
       " (1,\n",
       "  '0.402*\"trump\" + -0.241*\"you!\" + 0.227*\"carson\" + 0.227*\"rubio\" + -0.209*\"#makeamericagreatagain\" + -0.204*\"thank\" + 0.191*\"donald\" + 0.184*\"leads\" + 0.182*\"cruz\" + 0.164*\"@realdonaldtrump\"'),\n",
       " (2,\n",
       "  '-0.499*\"great\" + 0.278*\"trump\" + 0.268*\"#makeamericagreatagain\" + -0.242*\"new\" + -0.211*\"america\" + 0.193*\"#trump2016\" + -0.188*\"make\" + -0.180*\"again!\" + -0.169*\"big\" + 0.162*\"rubio\"'),\n",
       " (3,\n",
       "  '0.367*\"national\" + -0.316*\"great\" + -0.257*\"rubio\" + 0.226*\"#trump2016\" + 0.216*\"#makeamericagreatagain\" + 0.215*\"lead\" + -0.207*\"carson\" + -0.201*\"poll-\" + -0.191*\"trump\" + -0.184*\"you!\"'),\n",
       " (4,\n",
       "  '-0.260*\"national\" + -0.252*\"leads\" + 0.240*\"@realdonaldtrump\" + -0.224*\"donald\" + -0.204*\"great\" + 0.199*\"numbers\" + 0.189*\"@cnn\" + 0.168*\"you!\" + -0.153*\"america\" + 0.152*\"thank\"'),\n",
       " (5,\n",
       "  '0.323*\"rubio\" + 0.312*\"carson\" + -0.286*\"donald\" + 0.260*\"cruz\" + -0.195*\"clinton\" + -0.182*\"poll:\" + -0.177*\"leads\" + -0.177*\"@realdonaldtrump\" + 0.176*\"national\" + 0.150*\"bush\"'),\n",
       " (6,\n",
       "  '0.318*\"new\" + 0.265*\"@realdonaldtrump\" + -0.243*\"big\" + 0.213*\"great\" + -0.181*\"you!\" + -0.178*\"hillary\" + -0.175*\"clinton\" + 0.162*\"#1\" + -0.151*\"america\" + -0.143*\"leads\"'),\n",
       " (7,\n",
       "  '-0.276*\"debate\" + 0.235*\"poll.\" + -0.223*\"said\" + 0.210*\"iowa\" + 0.208*\"lead\" + 0.197*\"new\" + -0.192*\"debate.\" + 0.183*\"@cnn\" + -0.181*\"every\" + -0.155*\"great\"'),\n",
       " (8,\n",
       "  '0.345*\"@realdonaldtrump\" + -0.220*\"debate\" + 0.200*\"big\" + -0.198*\"last\" + 0.197*\"leads\" + -0.191*\"national\" + -0.189*\"lead\" + 0.173*\"leading\" + 0.151*\"#1\" + -0.148*\"said\"'),\n",
       " (9,\n",
       "  '-0.327*\"leads\" + 0.265*\"numbers\" + 0.253*\"lead\" + -0.207*\"new\" + -0.195*\"debate.\" + 0.167*\"clinton\" + -0.147*\"poll.\" + -0.146*\"said\" + 0.139*\"poll:\" + 0.136*\"hillary\"')]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lsi.print_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Topics\n",
    "\n",
    "We've transformed our corpus to have 10 topics according to ___LSI___:\n",
    "<br>\n",
    "**Topic 1**\n",
    "> _\"thank\", \"you!\", \"#makeamericagreatagain\", \"#trump2016\", \"great\", \"new\", \"poll\", \"-\", \"poll-\", \"poll\", \"national\"_\n",
    "\n",
    "It appears that ___\"thank\"___, ___\"you\"___, the hash-tags ___\"#makeamericagreatagain\"___ & ___\"#trump2016\"___ are all related and contribute the most in the direction of the first topic. While ___\"poll\"___ (with its variants) and ___\"national\"___ contribute the least.\n",
    "\n",
    "**Topic 2**\n",
    "> _\"trump\", \"you!\", \"carson\", \"rubio\", \"#makeamericagreatagain\", \"thank\", \"donald\", \"leads\", \"cruz\", \"@realdonaldtrump\"_\n",
    "\n",
    "In the second topic, we have the words ___\"trump\"___, ___\"you!\"___, ___\"carson\"___, ___\"rubio\"___ & the hash-tag ___\"#makeamericagreatagain\"___ contributing the most. This topic associates the current American president, [Donald Trump](https://en.wikipedia.org/wiki/Donald_Trump), with his Republican critics and opposition during the primaries, [Ben Carson](https://en.wikipedia.org/wiki/Ben_Carson) and [Marco Rubio](https://en.wikipedia.org/wiki/Marco_Rubio). While [Ted Cruz](https://en.wikipedia.org/wiki/Ted_Cruz) mentions contributed the least.\n",
    "\n",
    "**Topic 3**\n",
    "> _\"great\", \"trump\", \"#makeamericagreatagain\", \"new\", \"america\", \"#trump2016\", \"make\", \"again!\", \"big\", \"rubio\"_\n",
    "\n",
    "Here we have the president's name being associated with \"greatness\", \"bigness\" and \"America\". This is likely in association with his compaign slogan [\"Make America Great Again\"](https://en.wikipedia.org/wiki/Make_America_Great_Again).\n",
    "\n",
    "**Topic 4**\n",
    "> _\"national\", \"great\", \"rubio\", \"#trump2016\", \"#makeamericagreatagain\", \"lead\", \"poll-\", \"carson\", \"trump\", \"you!\"_\n",
    "\n",
    "Topic number 4 has a theme that contains \"national\", \"great\", \"Marco Rubio\", \"leading\", \"poll\", \"Ben Carson\", \"Donald Trump\" and the hash-tags ___\"#trump2016\"___ & ___\"#makeamericagreatagain\"___. This is likely the topic related to Donald Trump making comments about his Republican opposition and references to him leading in the national polls.\n",
    "\n",
    "**Topic 5**\n",
    "> _\"national\", \"leads\", \"@realdonaldtrump\", \"donald\", \"great\", \"numbers\", \"@cnn\", \"you!\", \"america\", \"thank\"_\n",
    "\n",
    "In this topic we have [Donald Trump](https://en.wikipedia.org/wiki/Donald_Trump) using his Twitter handle [___\"@realdonaldtrump\"___](https://twitter.com/realDonaldTrump) and tweeting about how he is leading in the national polls. There is reference to [___\"@cnn\"___](https://twitter.com/cnn) and giving thanks to the people of America.\n",
    "\n",
    "**Topic 6**\n",
    "> _\"rubio\", \"carson\", \"donald\", \"cruz\", \"clinton\", \"poll:\", \"leads\", \"national\", \"@realdonaldtrump\", \"bush\"_\n",
    "\n",
    "This topic contains tweets on topics where Donald Trump likely is comparing himself to his opposition, that is, [Marco Rubio](https://en.wikipedia.org/wiki/Marco_Rubio), [Ben Carson](https://en.wikipedia.org/wiki/Ben_Carson), [Ted Cruz](https://en.wikipedia.org/wiki/Ted_Cruz), [Hillary Clinton](https://en.wikipedia.org/wiki/Hillary_Clinton), and [Jeb Bush](https://en.wikipedia.org/wiki/Jeb_Bush), in the national polls.\n",
    "\n",
    "**Topic 7**\n",
    "> _\"new\", \"@realdonaldtrump\", \"big\", \"great\", \"you!\", \"clinton\", \"hillary\", \"#1\", \"america\", \"make\"_\n",
    "\n",
    "Topic 7 has [Donald Trump](https://en.wikipedia.org/wiki/Donald_Trump) tweeting about his Democrat opponent [Hillary Clinton](https://en.wikipedia.org/wiki/Hillary_Clinton) with references of \"#1\", \"new-ness\", \"big-ness\", \"great-ness\", \"making\" and \"America\". This is likely tweets where Donald Trump is critisizing Hillary, then making reference to his campaign slogan.\n",
    "\n",
    "**Topic 8**\n",
    "> _\"debate\", \"poll.\", \"said\", \"lead\", \"iowa\", \"new\", \"debate.\", \"@cnn\", \"@realdonaldtrump\", \"every\"_\n",
    "\n",
    "It appears in this topic, ___\"debate\"___ contributes the most. This is followed by lines (or sentences) ending with ___\"poll\"___, containing the words ___\"said\"___, ___\"lead\"___, ___\"Iowa\"___, ___\"new\"___, ___\"every\"___ and the CNN hash-tag ___\"@cnn\"___. [Iowa](https://en.wikipedia.org/wiki/Iowa) is referenced likely because of the presidential caucus, which is the first in the country and the starting point along with the [New Hampshire](https://en.wikipedia.org/wiki/New_Hampshire) primary, where the two major-party candidates for president are chosen.\n",
    "\n",
    "**Topic 9**\n",
    "> _\"@realdonaldtrump\", \"debate\", \"big\", \"leads\", \"national\", \"lead\", \"last\", \"leading\", \"#1\", \"said\"_\n",
    "\n",
    "Topic 9 associates Donald Trump's tweets on the debate and leading in the national polls. The official handle [***@realdonaldtrump***](https://twitter.com/realDonaldTrump) contributes the most in this topic, likely because these tweets were made using this account.\n",
    "\n",
    "**Topic 10**\n",
    "> _\"numbers\", \"leads\", \"lead\", \"new\", \"debate.\", \"said\", \"clinton\", \"poll.\", \"great\", \"big\"_\n",
    "\n",
    "In this topic, Donald Trump is tweeting about polling numbers, the debate and Hillary Clinton. These are likely tweets related to Donald Trump commenting and critizing what Hillary Clinton has said and how it has affected the polls."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Executing: bow->tfidf and tfidf->lsi\n",
    "for doc in corpus_lsi:\n",
    "    print(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-02-16 20:37:05,410 : INFO : saving Projection object under trump.lsi.projection, separately None\n",
      "2019-02-16 20:37:05,413 : INFO : saved trump.lsi.projection\n",
      "2019-02-16 20:37:05,414 : INFO : saving LsiModel object under trump.lsi, separately None\n",
      "2019-02-16 20:37:05,415 : INFO : not storing attribute projection\n",
      "2019-02-16 20:37:05,417 : INFO : not storing attribute dispatcher\n",
      "2019-02-16 20:37:05,418 : INFO : saved trump.lsi\n",
      "2019-02-16 20:37:05,419 : INFO : loading LsiModel object from trump.lsi\n",
      "2019-02-16 20:37:05,421 : INFO : loading id2word recursively from trump.lsi.id2word.* with mmap=None\n",
      "2019-02-16 20:37:05,422 : INFO : setting ignored attribute projection to None\n",
      "2019-02-16 20:37:05,423 : INFO : setting ignored attribute dispatcher to None\n",
      "2019-02-16 20:37:05,424 : INFO : loaded trump.lsi\n",
      "2019-02-16 20:37:05,425 : INFO : loading LsiModel object from trump.lsi.projection\n",
      "2019-02-16 20:37:05,426 : INFO : loaded trump.lsi.projection\n"
     ]
    }
   ],
   "source": [
    "# Model persistence: save(), load()\n",
    "lsi.save('trump.lsi')\n",
    "lsi = models.LsiModel.load('trump.lsi')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Similarity\n",
    "\n",
    "#### Step 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-02-16 20:37:07,422 : WARNING : scanning corpus to determine the number of features (consider setting `num_features` explicitly)\n",
      "2019-02-16 20:37:07,436 : INFO : creating matrix with 448 documents and 10 features\n"
     ]
    }
   ],
   "source": [
    "# Initializing the query structure: transform corpus to LSI space and index it\n",
    "index = similarities.MatrixSimilarity(lsi[corpus])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-02-16 20:37:08,360 : INFO : saving MatrixSimilarity object under trump.index, separately None\n",
      "2019-02-16 20:37:08,363 : INFO : saved trump.index\n",
      "2019-02-16 20:37:08,365 : INFO : loading MatrixSimilarity object from trump.index\n",
      "2019-02-16 20:37:08,367 : INFO : loaded trump.index\n"
     ]
    }
   ],
   "source": [
    "# Index persistence\n",
    "index.save('trump.index')\n",
    "index = similarities.MatrixSimilarity.load('trump.index')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweet Rank #1:\tWeight: 0.9700218439102173\n",
      "Raw text: Hillary Clinton is not a change agent, just the same old status quo! She is spending a fortune, I am spending very little. Close in polls!\n",
      "\n",
      "Tweet Rank #2:\tWeight: 0.959089994430542\n",
      "Raw text: Don't believe the @FoxNews Polls, they are just another phony hit job on me. I will beat Hillary Clinton easily in the General Election.\n",
      "\n",
      "Tweet Rank #3:\tWeight: 0.8974069356918335\n",
      "Raw text: .@USATODAY Poll and @QuinnipiacPoll say that I beat both Hillary and Bernie, and I havn't even started on them yet!\n",
      "\n",
      "Tweet Rank #4:\tWeight: 0.8799535036087036\n",
      "Raw text: The Republican establishment, out of self preservation, is concerned w/ my high poll #'s. More concerned are Demsâ€”I beat Hillary heads up!\n",
      "\n",
      "Tweet Rank #5:\tWeight: 0.8720756769180298\n",
      "Raw text: Kasich only looks O.K. in polls against Hillary because nobody views him as a threat and therefore have placed ZERO negative ads against him\n",
      "\n",
      "Tweet Rank #6:\tWeight: 0.8531491756439209\n",
      "Raw text: Ted Cruz is lying again. Polls are showing that I do beat Hillary Clinton head to head. Check out https://t.co/45g7qpxq7T Poll snd Q Poll.\n",
      "\n",
      "Tweet Rank #7:\tWeight: 0.8447518348693848\n",
      "Raw text: As soon as John Kasich is hit with negative ads, he will drop like a rock in the polls against Crooked Hillary Clinton. I will win!\n",
      "\n",
      "Tweet Rank #8:\tWeight: 0.8129469156265259\n",
      "Raw text: Hey @glennbeck- see how I beat your boy Ted- in your own Blaze poll? Your endorsement means nothing! #GOPDebate\n",
      "\n",
      "Tweet Rank #9:\tWeight: 0.8106362819671631\n",
      "Raw text: Respected Morning Consult poll just out. I lead all Republicans and beat Hillary head to head by a wide margin- 45 to 40!\n",
      "\n",
      "Tweet Rank #10:\tWeight: 0.764165997505188\n",
      "Raw text: Very honored: \"Trump Is Tops As Clinton Drops In Connecticut Primaries, Quinnipiac University Poll Finds\"     \n",
      "http://t.co/6d8WiLa8fB\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Performing queries\n",
    "doc = \"Hillary Clinton.\"\n",
    "vec_bow = bag.doc2bow(doc.lower().split())\n",
    "\n",
    "# Convert the query to LSI space\n",
    "vec_lsi = lsi[vec_bow]\n",
    "\n",
    "# Perform a similarity query against the corpus\n",
    "sims = index[vec_lsi]\n",
    "\n",
    "# Ranking the tweets by their weights of similarity\n",
    "sims = sorted(enumerate(sims), key=lambda item: -item[1])\n",
    "\n",
    "# Printing the associated Tweets:\n",
    "for i in range(10):\n",
    "    print(\"Tweet Rank #{}:\\tWeight: {}\\nRaw text: {}\\n\".format(i+1, sims[i][1], document[sims[i][0]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we make a query for \"Hillary Clinton\" to retrieve the respective top tweets associated with her name, we find that the leading tweet with the greatest weight is a strong criticism of Hillary Clinton and her campaign spending. The remaining nine tweets are associated with the polls, darted with references to Donald Trump criticizing poll results not in his favor, him leading against the opposition as well as pitting himself likely to win."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweet Rank #1:\tWeight: 0.9365328550338745\n",
      "Raw text: Just out: Boston Herald/Franklin Pierce Poll  N.H.  TRUMP 28 (up 10)  CARSON 16  BUSH 9  RUBIO 6  CRUZ 5  Press will say they are surging!\n",
      "\n",
      "Tweet Rank #2:\tWeight: 0.9363357424736023\n",
      "Raw text: RT @DRUDGE_REPORT: REUTERS ROLLING:  TRUMP 39%, CRUZ 14.5%, BUSH 10.6%, CARSON 9.6%, RUBIO 6.7%... MORE... https://t.co/nRhtbzcqP9\n",
      "\n",
      "Tweet Rank #3:\tWeight: 0.9350287318229675\n",
      "Raw text: Jeb Bush just got contact lenses and got rid of the glasses. He wants to look cool, but it's far too late. 1% in Nevada!\n",
      "\n",
      "Tweet Rank #4:\tWeight: 0.9347569346427917\n",
      "Raw text: 2016 Republican Primary Morning Consult Poll was just released. TRUMP 32, CARSON 12, BUSH 11, FIORINA 6, RUBIO 5, CRUZ 5. Taken after debate\n",
      "\n",
      "Tweet Rank #5:\tWeight: 0.9299113154411316\n",
      "Raw text: Tennessee GOP Poll\n",
      "https://t.co/hfSxKaXC42\n",
      "Trump 32.7%\n",
      "Cruz 16.5%\n",
      "Carson 6.6%\n",
      "Rubio 5.3%\n",
      "Christie 2.4%\n",
      "Jeb 1.6%\n",
      "\n",
      "Tweet Rank #6:\tWeight: 0.9277924299240112\n",
      "Raw text: CNN/ORC Poll results just out for Nevadaâ€”WOW! Trump 38, Carson 22, Fiorina 8, Bush 6, Cruz 4\n",
      "  http://t.co/X2Kv6qzPia\n",
      "\n",
      "Tweet Rank #7:\tWeight: 0.9228523373603821\n",
      "Raw text: \"@SassyPantsjj: Michigan GOP poll 2/24/2016 Trump 35.5 Rubio 15.0 Cruz 14.3 Kasich 12.8 Carson 8.3 @realDonaldTrump\" Wow!\n",
      "\n",
      "Tweet Rank #8:\tWeight: 0.9069275856018066\n",
      "Raw text: Thank you Michigan! #VoteTrumpMI\n",
      "Trump 35%\n",
      "Kasich 17%\n",
      "Cruz 12%\n",
      "Rubio 12%\n",
      "Carson 9% \n",
      "Via: ARG\n",
      "\n",
      "Tweet Rank #9:\tWeight: 0.9036007523536682\n",
      "Raw text: @CNBC  POLL  TOTAL: TRUMP 25.22  CARSON 19.78  RUBIO 9.67....\n",
      "\n",
      "Tweet Rank #10:\tWeight: 0.8996983766555786\n",
      "Raw text: The polls show that I picked up many Jeb Bush supporters. That is how I got to 46%. When others drop out, I will pick up more.  Sad but true\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Performing queries\n",
    "doc = \"Ben Carson\"\n",
    "vec_bow = bag.doc2bow(doc.lower().split())\n",
    "\n",
    "# Convert the query to LSI space\n",
    "vec_lsi = lsi[vec_bow]\n",
    "\n",
    "# Perform a similarity query against the corpus\n",
    "sims = index[vec_lsi]\n",
    "\n",
    "# Ranking the tweets by their weights of similarity\n",
    "sims = sorted(enumerate(sims), key=lambda item: -item[1])\n",
    "\n",
    "# Printing the associated Tweets:\n",
    "for i in range(10):\n",
    "    print(\"Tweet Rank #{}:\\tWeight: {}\\nRaw text: {}\\n\".format(i+1, sims[i][1], document[sims[i][0]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(98, 0.93633574)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sims[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": false,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
