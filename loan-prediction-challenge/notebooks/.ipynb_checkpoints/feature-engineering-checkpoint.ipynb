{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loan Prediction Challenge: Feature Engineering\n",
    "\n",
    "In this notebook, we will carry out feature engineering on the datasets and create a feature matrix for training and testing.\n",
    "\n",
    "## Table of Contents\n",
    "* **[Preprocessing](#preprocessing)**\n",
    "  * [Join Datasets](#join-datasets)\n",
    "  * [Entities and Entitysets](#entities-and-entitysets)\n",
    "   * [Adding Entities](#adding-entities)\n",
    "  * [Relationships](#relationships)\n",
    "   * [Adding Relationships](#adding-relationships)\n",
    "   * [Visualise `EntitySet`](#visualise-entityset)\n",
    "  * [Feature Primitives](#feature-primitives)\n",
    "* **[Deep Feature Synthesis (DFS)](#deep-feature-synthesis)**\n",
    "  * [Selecting Primitives](#selecting-primitives)\n",
    "  * [Run Full Deep Feature Synthesis](#run-full-deep-feature-synthesis)\n",
    "  * [Save](#save)\n",
    "* **[References](#references)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Header\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "from __future__ import unicode_literals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The usual suspects ...\n",
    "import sys\n",
    "import psutil\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import featuretools as ft\n",
    "import featuretools.variable_types as vtypes\n",
    "\n",
    "from scipy import stats\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading in the data\n",
    "# Training datasets:\n",
    "traindemographics = pd.read_csv('../data/traindemographics.csv')\n",
    "trainperf = pd.read_csv('../data/trainperf.csv')\n",
    "trainprevloans = pd.read_csv('../data/trainprevloans.csv')\n",
    "\n",
    "# Testing datasets:\n",
    "testdemographics = pd.read_csv('../data/testdemographics.csv')\n",
    "testperf = pd.read_csv('../data/testperf.csv')\n",
    "testprevloans = pd.read_csv('../data/testprevloans.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='preprocessing'></a>\n",
    "## Preprocessing\n",
    "\n",
    "<a id='join-datasets'></a>\n",
    "### Join Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create target variable for test dataset\n",
    "testperf['good_bad_flag'] = np.nan\n",
    "\n",
    "# Join train and test datasets\n",
    "demographics = traindemographics.append(testdemographics, ignore_index=True, sort=True)\n",
    "performance = trainperf.append(testperf, ignore_index=True, sort=True)\n",
    "prevloans = trainprevloans.append(testprevloans, ignore_index=True, sort=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Index(['bank_account_type', 'bank_branch_clients', 'bank_name_clients',\n",
       "        'birthdate', 'customerid', 'employment_status_clients', 'latitude_gps',\n",
       "        'level_of_education_clients', 'longitude_gps'],\n",
       "       dtype='object'),\n",
       " Index(['approveddate', 'creationdate', 'customerid', 'good_bad_flag',\n",
       "        'loanamount', 'loannumber', 'referredby', 'systemloanid', 'termdays',\n",
       "        'totaldue'],\n",
       "       dtype='object'),\n",
       " Index(['approveddate', 'closeddate', 'creationdate', 'customerid',\n",
       "        'firstduedate', 'firstrepaiddate', 'loanamount', 'loannumber',\n",
       "        'referredby', 'systemloanid', 'termdays', 'totaldue'],\n",
       "       dtype='object'))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "demographics.columns, performance.columns, prevloans.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After combining the dat sets, the performance and prevloans datasets have the `systemloanid` as the unique identifier. On the other hand, the demographics dataset has no unique identifier. The `customerid` seems like a possible candidate for an index but it contains duplicate entries. We'll need to create a unique index for this dataset and give it a name when we create entities.\n",
    "\n",
    "<a id='entities-and-entitysets'></a>\n",
    "### Entities and Entitysets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create entityset\n",
    "entityset = ft.EntitySet(id='customers')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='adding-entities'></a>\n",
    "#### Adding Entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Entityset: customers\n",
       "  Entities:\n",
       "    loan_performance [Rows: 5818, Columns: 10]\n",
       "    previous_loans [Rows: 24090, Columns: 12]\n",
       "    customer_demographics [Rows: 5833, Columns: 10]\n",
       "  Relationships:\n",
       "    No relationships"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Entities with a unique index\n",
    "entityset = entityset.entity_from_dataframe(entity_id='loan_performance',\n",
    "                                            dataframe=performance,\n",
    "                                            index='customerid')\n",
    "entityset = entityset.entity_from_dataframe(entity_id='previous_loans',\n",
    "                                            dataframe=prevloans,\n",
    "                                            index='systemloanid')\n",
    "\n",
    "# Entities with no unique index\n",
    "entityset = entityset.entity_from_dataframe(entity_id='customer_demographics',\n",
    "                                            dataframe=demographics,\n",
    "                                            make_index=True,\n",
    "                                            index='demographicid')\n",
    "\n",
    "# Show entityset\n",
    "entityset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='relationships'></a>\n",
    "### Relationships\n",
    "\n",
    "From the dataset, performance seems to be the parent table with two unique identifiers: `customerid` and `systemloanid`. The demographics and prevloans datasets are child tables of performance dataset since the performance dataset has one row for each customer, while demographics and prevloans have multiple entries.\n",
    "\n",
    "<a id='adding-relationships'></a>\n",
    "#### Adding Relationships\n",
    "\n",
    "For each relationship, we need to first specify the parent variable and then the child variable. Using an `EntitySet` that tracks the relationships will allow us to work at a higher level of abstraction, thinking about the entire dataset rather than each individual table. This will greatly increase our efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Relationship between performance and demographics - `customerid`\n",
    "r_perf_demo = ft.Relationship(entityset['loan_performance']['customerid'], \n",
    "                              entityset['customer_demographics']['customerid'])\n",
    "\n",
    "# Relationship between performance and previous loans - `customerid`\n",
    "r_perf_prev = ft.Relationship(entityset['loan_performance']['customerid'],\n",
    "                              entityset['previous_loans']['customerid'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Entityset: customers\n",
       "  Entities:\n",
       "    loan_performance [Rows: 5818, Columns: 10]\n",
       "    previous_loans [Rows: 24090, Columns: 12]\n",
       "    customer_demographics [Rows: 5833, Columns: 10]\n",
       "  Relationships:\n",
       "    customer_demographics.customerid -> loan_performance.customerid\n",
       "    previous_loans.customerid -> loan_performance.customerid"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add the defined relationships\n",
    "entityset = entityset.add_relationships([r_perf_demo, r_perf_prev])\n",
    "\n",
    "# Show entityset\n",
    "entityset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:**\n",
    "\n",
    "We need to be careful not to create a [diamond graph](https://en.wikipedia.org/wiki/Diamond_graph) where there are multiple paths from a parent to a child - this results in ambiguity.\n",
    "\n",
    "\n",
    "All entities in the entity can be linked through these relationships. In theory, we should be able to calculate features for any of the entities. However, in practice, we will only calculate features for the parent dataframe that will be used for training/testing. The end outcome will be a dataframe that has one row for each client in the parent with thousands of features for each individual.\n",
    "\n",
    "<a id='visualise-entityset'></a>\n",
    "#### Visualise `EntitySet`"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "entityset.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='feature-primitives'></a>\n",
    "### Feature Primitives\n",
    "\n",
    "A [feature primitive](https://docs.featuretools.com/automated_feature_engineering/primitives.html) is an operation applied to a table or a set of tables to create a feature. These represent simple calculations, most of which are already used in manual feature engineering, that can be stacked on top of each other to create complex deep features. Feature primitives fall into two categories:\n",
    "\n",
    "- **Aggregation**: a function that groups together children for each parent and calculates a statistic such as the mean, min, max, or standard deviation across the children. An example is the maximum previous loan amount for each client. An aggregation covers multiple tables using relationships between tables.\n",
    "\n",
    "- **Transformation**: an operation applied to one or more columns in a single table. An example would be taking the absolute value of a column, or finding the difference between two columns in one table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>type</th>\n",
       "      <th>description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>all</td>\n",
       "      <td>aggregation</td>\n",
       "      <td>Calculates if all values are 'True' in a list.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>percent_true</td>\n",
       "      <td>aggregation</td>\n",
       "      <td>Determines the percent of `True` values.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>last</td>\n",
       "      <td>aggregation</td>\n",
       "      <td>Determines the last value in a list.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>num_unique</td>\n",
       "      <td>aggregation</td>\n",
       "      <td>Determines the number of distinct values, ignoring `NaN` values.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>skew</td>\n",
       "      <td>aggregation</td>\n",
       "      <td>Computes the extent to which a distribution differs from a normal distribution.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>min</td>\n",
       "      <td>aggregation</td>\n",
       "      <td>Calculates the smallest value, ignoring `NaN` values.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>avg_time_between</td>\n",
       "      <td>aggregation</td>\n",
       "      <td>Computes the average number of seconds between consecutive events.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>mean</td>\n",
       "      <td>aggregation</td>\n",
       "      <td>Computes the average for a list of values.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>trend</td>\n",
       "      <td>aggregation</td>\n",
       "      <td>Calculates the trend of a variable over time.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>count</td>\n",
       "      <td>aggregation</td>\n",
       "      <td>Determines the total number of values, excluding `NaN`.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               name         type  \\\n",
       "0               all  aggregation   \n",
       "1      percent_true  aggregation   \n",
       "2              last  aggregation   \n",
       "3        num_unique  aggregation   \n",
       "4              skew  aggregation   \n",
       "5               min  aggregation   \n",
       "6  avg_time_between  aggregation   \n",
       "7              mean  aggregation   \n",
       "8             trend  aggregation   \n",
       "9             count  aggregation   \n",
       "\n",
       "                                                                       description  \n",
       "0                                   Calculates if all values are 'True' in a list.  \n",
       "1                                         Determines the percent of `True` values.  \n",
       "2                                             Determines the last value in a list.  \n",
       "3                 Determines the number of distinct values, ignoring `NaN` values.  \n",
       "4  Computes the extent to which a distribution differs from a normal distribution.  \n",
       "5                            Calculates the smallest value, ignoring `NaN` values.  \n",
       "6               Computes the average number of seconds between consecutive events.  \n",
       "7                                       Computes the average for a list of values.  \n",
       "8                                    Calculates the trend of a variable over time.  \n",
       "9                          Determines the total number of values, excluding `NaN`.  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Listing the primitives in a dataframe\n",
    "primitives = ft.list_primitives()\n",
    "pd.options.display.max_colwidth = 100\n",
    "\n",
    "primitives[primitives['type'] == 'aggregation'].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>type</th>\n",
       "      <th>description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>cum_mean</td>\n",
       "      <td>transform</td>\n",
       "      <td>Calculates the cumulative mean.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>modulo_by_feature</td>\n",
       "      <td>transform</td>\n",
       "      <td>Return the modulo of a scalar by each element in the list.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>subtract_numeric_scalar</td>\n",
       "      <td>transform</td>\n",
       "      <td>Subtract a scalar from each element in the list.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>less_than_equal_to_scalar</td>\n",
       "      <td>transform</td>\n",
       "      <td>Determines if values are less than or equal to a given scalar.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>cum_sum</td>\n",
       "      <td>transform</td>\n",
       "      <td>Calculates the cumulative sum.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>divide_by_feature</td>\n",
       "      <td>transform</td>\n",
       "      <td>Divide a scalar by each value in the list.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>modulo_numeric</td>\n",
       "      <td>transform</td>\n",
       "      <td>Element-wise modulo of two lists.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>less_than</td>\n",
       "      <td>transform</td>\n",
       "      <td>Determines if values in one list are less than another list.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>less_than_scalar</td>\n",
       "      <td>transform</td>\n",
       "      <td>Determines if values are less than a given scalar.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>minute</td>\n",
       "      <td>transform</td>\n",
       "      <td>Determines the minutes value of a datetime.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         name       type  \\\n",
       "20                   cum_mean  transform   \n",
       "21          modulo_by_feature  transform   \n",
       "22    subtract_numeric_scalar  transform   \n",
       "23  less_than_equal_to_scalar  transform   \n",
       "24                    cum_sum  transform   \n",
       "25          divide_by_feature  transform   \n",
       "26             modulo_numeric  transform   \n",
       "27                  less_than  transform   \n",
       "28           less_than_scalar  transform   \n",
       "29                     minute  transform   \n",
       "\n",
       "                                                       description  \n",
       "20                                 Calculates the cumulative mean.  \n",
       "21      Return the modulo of a scalar by each element in the list.  \n",
       "22                Subtract a scalar from each element in the list.  \n",
       "23  Determines if values are less than or equal to a given scalar.  \n",
       "24                                  Calculates the cumulative sum.  \n",
       "25                      Divide a scalar by each value in the list.  \n",
       "26                               Element-wise modulo of two lists.  \n",
       "27    Determines if values in one list are less than another list.  \n",
       "28              Determines if values are less than a given scalar.  \n",
       "29                     Determines the minutes value of a datetime.  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "primitives[primitives['type'] == 'transform'].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='deep-feature-synthesis'></a>\n",
    "## Deep Feature Synthesis (DFS)\n",
    "\n",
    "[Deep Feature Synthesis (DFS)](https://www.featuretools.com/blog/deep-feature-synthesis) is the method Featuretools uses to make new features. DFS stacks feature primitives to form features with a \"depth\" equal to the number of primitives.\n",
    "\n",
    "**Example**\n",
    "- The maximum value of a client's previous loans, `MAX(prevloans.loan_amount)`, is a \"deep feature\" with a depth of 1.\n",
    "- To create a feature of depth 2, we would stack primitives by taking the maximum value of a client's average monthly payments per previous loan, `MAX(prevloans(MEAN(installments.payment)))`. In manual feature engineering, this would require two seperate groupings and aggregations, and take more than 15 minutes to code per feature.\n",
    "\n",
    "**Advantages of DFS**\n",
    "- Allows us to overcome human limitations of time and creativity by building features that we would never be able to think of on our own, or would not have the patience to implement.\n",
    "- DFS is applicable to any dataset with only very minor changes in syntax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Built 107 features\n"
     ]
    }
   ],
   "source": [
    "# Default primitives from featuretools\n",
    "default_agg_primitives = ['sum', 'std', 'max', 'skew', 'min', 'mean', 'count', 'percent_true', 'num_unique', 'mode']\n",
    "default_trans_primitives = ['day', 'year', 'month', 'weekday', 'haversine', 'num_words', 'num_characters']\n",
    "\n",
    "# DFS with specified primitives\n",
    "feature_names = ft.dfs(entityset=entityset,\n",
    "                       target_entity='loan_performance',\n",
    "                       trans_primitives=default_trans_primitives,\n",
    "                       agg_primitives=default_agg_primitives,\n",
    "                       where_primitives=[],\n",
    "                       seed_features=[],\n",
    "                       max_depth=2,\n",
    "                       n_jobs=-1,\n",
    "                       verbose=1,\n",
    "                       features_only=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='selecting-primitives'></a>\n",
    "### Selecting Primitives\n",
    "\n",
    "For our actual set of features, we will use a select group of primitives rather than just the defaults."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify primitives\n",
    "agg_primitives = ['sum', 'max', 'min', 'mean', 'count', 'percent_true', 'num_unique', 'mode', 'median']\n",
    "trans_primitives = ['year', 'month', 'day', 'percentile', 'and']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Built 162 features\n"
     ]
    }
   ],
   "source": [
    "# DFS\n",
    "feature_names = ft.dfs(entityset=entityset,\n",
    "                       target_entity='loan_performance',\n",
    "                       agg_primitives=agg_primitives,\n",
    "                       trans_primitives=trans_primitives,\n",
    "                       n_jobs=-1,\n",
    "                       verbose=1,\n",
    "                       features_only=True,\n",
    "                       max_depth=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the features. We'll want to use them later on a seperate dataset\n",
    "ft.save_features(feature_names, '../data/features.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='run-full-deep-feature-synthesis'></a>\n",
    "### Run Full Deep Feature Synthesis\n",
    "\n",
    "If we are content with the features that have been built, we can run DFS and create the feature matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total size of entityset: 0.01 GB\n",
      "Total number of CPUs detected: 4\n",
      "Total size of system memory: 7.76 GB\n"
     ]
    }
   ],
   "source": [
    "print('Total size of entityset: {:.2f} GB'.format(sys.getsizeof(entityset) / 1e9))\n",
    "print('Total number of CPUs detected: {}'.format(psutil.cpu_count()))\n",
    "print('Total size of system memory: {:.2f} GB'.format(psutil.virtual_memory().total / 1e9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Built 162 features\n",
      "Elapsed: 03:45 | Progress:  58%|█████▊    | Remaining: 03:11"
     ]
    }
   ],
   "source": [
    "# Run DFS\n",
    "feature_matrix, feature_names = ft.dfs(entityset=entityset,\n",
    "                                       target_entity='loan_performance',\n",
    "                                       agg_primitives=agg_primitives,\n",
    "                                       trans_primitives=trans_primitives,\n",
    "                                       n_jobs=1,\n",
    "                                       verbose=1,\n",
    "                                       features_only=False,\n",
    "                                       max_depth=2,\n",
    "                                       chunk_size=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='save'></a>\n",
    "### Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reformat\n",
    "feature_matrix.reset_index(inplace=True)\n",
    "\n",
    "# Save\n",
    "feature_matrix.to_csv('../data/feature_matrix.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='references'></a>\n",
    "## References\n",
    "\n",
    "- [Deep Feature Synthesis - original paper](https://dai.lids.mit.edu/wp-content/uploads/2017/10/DSAA_DSM_2015.pdf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
